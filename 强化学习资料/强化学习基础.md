# 强化学习基础

本篇笔记是学习B站 `王树森` 老师的深度强化学习系列课程总结的，再次向其致谢~

## 基本概念

### 奖励回报

回报定义为未来的累计奖励之和，记作 $U_t=R_t+R_{t+1}+R_{t+2}+R_{t+3}+...$

强化学习中一般采用累计折扣回报，记作 $U_t=R_t+\gamma R_{t+1}+{\gamma^2} R_{t+2}+{\gamma^3} R_{t+3}+...$，其中 $\gamma$ 表示折扣因子，是一个可以调整的超参数

在时间点 $t$，累计折扣回报 $U_t$ 具有随机性，随机性主要有两个来源

- 在当前状态下可以采取的动作是随机的
- 下一个时间点的状态也是未知的

对于任意的时刻 $i\geq{t}$，奖励 $R_i$ 依赖于状态 $S_i$ 和 动作 $A_i$，因此，在给定 $s_t$ 的情况下，累计折扣回报 $U_t$ 依赖于随机变量 $A_t,A_{t+1},A_{t+2},...$  和 $S_t,S_{t+1},S_{t+2},...$ 

### 价值函数

#### 动作价值函数

动作价值函数记作 $Q_\pi{(s_t, a_t)}$,  $Q_\pi{(s_t, a_t)}=E[U_t|(S_t=s_t,A_t=a_t)]$ ，随机变量 $A_t,A_{t+1},A_{t+2},...$  和 $S_t,S_{t+1},S_{t+2},...$ 在求期望的过程中会被消掉，所以，动作价值函数只与当前状态和当前动作有关

**直观意义：**在给定策略函数 $\pi$ 的情况下，动作价值函数会计算出当前状态下每一个动作的价值，以此来评价动作的好坏

最优动作价值函数是指找到一个最优的策略函数使得动作价值函数 $Q_\pi$ 最大化，可以表示为$Q^\star(s_t,a_t)=max_{\pi}Q_\pi(s_t,a_t)$ ，最优动作价值函数 $Q^\star(s_t,a_t)$ 和策略函数 $\pi{(a|s)}$ 无关，

#### 状态价值函数

状态价值函数是对动作价值函数中的随机变量 $A$ 求期望，以消掉随机变量 $A$ ，状态价值函数只与状态 $s$ 和 策略 $\pi$ 有关，可表示为如下式：

- 离散动作： $V_\pi(s_t)=E_A[Q_\pi(s_t,A)]=\Sigma_a{\pi(a|s_t)\cdot{Q_\pi(s_t,a)}}$

- 连续动作：$V_\pi(s_t)=E_A[Q_\pi(s_t,A)]=\int{\pi(a|s_t)\cdot{Q_\pi(s_t,a)}da}$

### 对价值函数的理解

- 动作价值函数：$Q_\pi(s_t,a_t)=E[U_t|S_t=s_t,A_t=a_t]$，对于策略 $\pi$ 而言，动作价值函数 $Q_\pi(s,a)$ 评估了在状态 $s$ 下，智能体选择的动作 $a$ 的好坏
- 状态价值函数：$V_\pi(s)=E_A[Q_\pi(s,A)]$，对于一个确定的策略 $\pi$ 而言，$V_\pi(s)$ 评估了当前状态 $s$ 的好坏，$E_s[V_\pi(s)]$ 评估了当前策略 $\pi$ 的好坏

### AI如何控制智能体

- 假设学习到了一个好的策略网络 $\pi(a|s)$，在观测到当前状态 $s_t$ 的情况下，随机采样得到动作 $a_t$ 
-  假设学习到了一个最优的动作价值函数 $Q^\star(s,a)$，在观测到状态 $s_t$ 的情况下，选择最大动作价值对应的动作， $a_t=argmax_aQ^\star(s_t,a)$

>强化学习的任务就是学习策略函数或最优的动作价值函数

## 价值学习

价值学习的关键是近似表示出最优的动作价值函数，当确定出最优动作价值函数 $Q^\star(s,a)$ 时，最好的动作为： $a^\star=argmax_aQ^\star(s,a)$，问题的关键即转化为如何确定 $Q^\star(s,a)$

**解决方案**是使用一个神经网络 $Q(s,a;\omega)$ 去近似最优动作价值函数 $Q^\star(s,a)$

### 时序差分 (Temporal Difference, TD)

$Q(s_t,a_t;\omega)\approx{r_t}+\gamma{\cdot}Q(s_{t+1},a_{t+1};\omega)$

>$\because$  $Q(s_t,a_t;\omega)$ 是 $E[U_t]$ 的估计， $Q(s_{t+1},a_{t+1};\omega)$ 是 $E[U_{t+1}]$ 的估计，且 $U_t=R_t+\gamma {U_{t+1}}$
>
>$\therefore$  $Q(s_t,a_t;\omega)\approx{r_t}+\gamma{\cdot}Q(s_{t+1},a_{t+1};\omega)$

### 使用 TD 算法训练 DQN

- 得到当前状态 $S_t=s_t$ 和当前动作 $A_t=a_t$
- 得到动作价值函数的预测值 $q_t=Q(s_t,a_t;\omega_t)$
- 计算价值网络的差分 $d_t=\frac{\partial{Q(s_t,a_t;\omega)}}{\partial{\omega}}|_{\omega=\omega_t}$
- 从外部环境中得到新的状态 $s_{t+1}$ 和 奖励 $r_t$
- 计算 `TD Target` ：$y_t=r_t+\gamma\cdot{max_aQ(s_{t+1},a;\omega_t)}$
- 使用梯度下降更新网络参数：$\omega_{t+1}=\omega_t-\alpha\cdot{(q_t-y_t){\cdot}{d_t}}$

## 策略学习

策略函数  $\pi(a|s)$  是一个概率分布函数，它的输入是状态 $s$ ，输出是动作的概率分布，智能体按照策略函数的概率分布做随机抽样得到动作 $a$

### 策略网络

使用一个神经网络来近似表示策略函数，这个神经网络被称为策略网络，记作 $\pi(a|s, \theta)$，其中，$\theta$ 代表了神经网络中可以训练的参数

状态价值函数：$V_\pi(s_t)=E_A[Q_\pi(s_t,A)]=\Sigma_a{\pi(a|s_t)\cdot{Q_\pi(s_t,a)}}$，如果使用神经网络 $\pi(a|s_t;\theta)$ 来近似策略函数 $\pi(a|s_t)$，那么状态价值函数可以近似为：$V(s_t;\theta)=\Sigma_a{\pi(a|s_t;\theta)\cdot{Q_\pi(s_t,a)}}$

定义：$J(\theta)=E_s[V(S;\theta)]$ ，策略学习通过学习参数 $\theta$ 来使得 $J(\theta)$ 最大化，可以使用梯度上升的方法来更新参数$\theta$，观测到状态 $s$ 后，通过 $\theta\leftarrow{\theta+\beta\cdot{\frac{\partial{V(s;\theta)}}{\partial{\theta}}}}$ 来更新参数 $\theta$，其中 ${\frac{\partial{V(s;\theta)}}{\partial{\theta}}}$ 被称作**策略梯度**，策略梯度的求导过程可写做如下：

 ${\frac{\partial{V(s;\theta)}}{\partial{\theta}}}$= $\frac{\partial{\Sigma_a{\pi(a|s;\theta)}{\cdot}Q_\pi(s,a)}}{\partial\theta}$=$\Sigma_a{\frac{\partial\pi(a|s;\omega)}{\partial\theta}}{\cdot}Q_\pi(s,a)$=$\Sigma_a\pi(a|s;\theta){\cdot}\frac{\partial\log\pi(a,s;\theta)}{\partial\theta}{\cdot}Q_\pi(s,a)$=$E_A[\frac{\partial\log\pi(A|s;\theta)}{\partial\theta}{{\cdot}Q_\pi(s,A)}]$

### 离散动作

如果动作是离散的，使用策略梯度 ${\frac{\partial{V(s;\theta)}}{\partial{\theta}}}=\Sigma_a{\frac{\partial\pi(a|s;\omega)}{\partial\theta}}{\cdot}Q_\pi(s,a)$，计算步骤如下：

- 计算出每一个离散动作的策略梯度：$f(a,\theta)=\frac{\partial\pi(a|s;\theta)}{\partial\theta}{\cdot}Q_\pi(s,a)$
- 计算整体的策略梯度：${\frac{\partial{V(s;\theta)}}{\partial{\theta}}}=f(a_1,\theta)+f(a_1,\theta)+...+f(a_n,\theta)$

### 连续动作

如果动作是连续的，使用策略梯度 ${\frac{\partial{V(s;\theta)}}{\partial{\theta}}}=E_{A\sim\pi(\cdot|s;\theta)}[\frac{\partial\log\pi(A|s;\theta)}{\partial\theta}{{\cdot}Q_\pi(s,A)}]$，计算步骤如下：

- 通过策略函数随机抽样得到动作  $\widehat{a}$
- 计算 $g(\widehat{a},\theta)=\frac{\partial\log\pi(\widehat{a}|s;\theta)}{\partial\theta}{{\cdot}Q_\pi(s,\widehat{a})}$
- 使用 $g(\widehat{a},\theta)$ 近似策略梯度 ${\frac{\partial{V(s;\theta)}}{\partial{\theta}}}$

### 策略梯度算法流程

- 观测到状态 $s_t$
- 使用策略函数随机采样得到动作 $a_t$
- 计算动作价值 $q_t{\approx}Q_\pi(s_t,a_t)$
- 计算策略网络的梯度：$d_{\theta,t}=\frac{\partial\log(a_t|s_t,\theta)}{\partial\theta}|_{\theta=\theta_t}$
- 计算近似的策略梯度：$g(a_t,\theta_t)=q_t{\cdot}d_{\theta,t}$
- 更新策略网络：$\theta_{t+1}=\theta_t+\beta{\cdot{g(a_t,\theta_t)}}$

### 如何计算动作价值 $Q_\pi(s_t,a_t)$

- Reinforce 方法
  - 玩一整局游戏产生轨迹集合：$s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T$
  - 对所有时刻，计算折扣奖励 $u_t=\Sigma_{k=t}^T\gamma^{k-t}r_k$
  - 因为 $Q_\pi(s_t,a_t)=E[U_t]$，所以可以使用 $u_t$ 来近似 $Q_\pi(s_t,a_t)$，即 $q_t=u_t$
- 使用神经网络来近似动作价值函数 $Q_\pi$，也就是之后的 Actor-Critic 方法

## Actor-Critic

使用神经网络 $\pi(a|s;\theta)$ 来近似策略函数 $\pi(a|s)$，称之为 `actor` 网络，使用神经网络 $q(s,a;\omega)$ 来近似动作价值函数 $Q_\pi(s,a)$，称之为 `critic` 网络，因此，状态价值函数可以表示为：

$V_\pi(s)=\Sigma_a\pi(a|s){\cdot}Q_\pi(s,a)\approx\Sigma_a\pi(a|s;\theta){\cdot}q(s,a;\omega)$

> 训练目标是更新策略网络和价值网络的参数 $\theta$ 和 $\omega$
>
> 更新策略网络 $\pi(a|s,\theta)$ 使得状态价值 $V(s;\theta,\omega)$ 增加
>
> 更新价值网络 $q(s,a;\omega)$ 以得到更好的奖励回报

更新的步骤如下：

- 观测到状态 $s_t$
- 通过策略函数 $\pi({\cdot}|s_t;\theta_t)$ 随机抽取动作 $a_t$
- 执行动作 $a_t$ 得到下一个状态 $s_{t+1}$ 和奖励回报 $r_t$
- 使用TD算法更新价值网络参数 $\omega$
- 使用策略梯度算法更新策略网络参数 $\theta$

### 训练价值网络

训练价值网络的步骤如下：

- 计算 $q(s_t,a_t;\omega_t)$ 和 $q(s_{t+1},a_{t+1};\omega_t)$
- 计算 `TD Target`：$y_t=r_t+\gamma{\cdot}q(s_{t+1},a_{t+1};\omega_t)$
- 计算损失函数：$L(\omega)=\frac{1}{2}*[q(s_t,a_t;\omega)-y_t]^2$
- 使用梯度下降法更新参数：$\omega_{t+1}=\omega_{t}-\alpha{\cdot}\frac{\partial{L(\omega)}}{\partial\omega}|_{\omega=\omega_t}$

### 训练策略网络

- 定义 $g(a,\theta)=\frac{\partial\log\pi(a|s;\theta)}{\partial\theta}{{\cdot}q(s_t,a;\omega)}$，则 ${\frac{\partial{V(s;\theta,\omega_t)}}{\partial{\theta}}}=E_A(g(A,\theta))$
- 按照策略网络随机采样动作 $a\sim\pi(\cdot|s_t;\theta_t)$ ，此时 $g(a,\theta)$ 是 $g(A,\theta)$ 的无偏估计
- 使用随机梯度上升算法可得：$\theta_{t+1}=\theta_t+\beta{\cdot}g(a,\theta_t)$

`Actor-Critic` 算法的更新示意图如下：

![ac](./images\ac.png)

### Actor-Critic算法步骤总结

- 观测到状态 $s_t$ 并随机采样动作 $a_t\sim\pi(\cdot|s_t;\theta_t)$
- 执行动作 $a_t$ ，并从环境中得到下一个状态 $s_{t+1}$ 和奖励 $r_t$
- 随机采样动作 $a_{t+1}\sim\pi(\cdot|s_{t+1};\theta_t)$，$a_{t+1}$ 并不真正执行
- 利用价值网络得到 $q_t=q(s_t,a_t;\omega_t)$ 和 $q_{t+1}=q(s_{t+1},a_{t+1};\omega_t)$
- 计算 `TD error`： $\delta_t=q_t-(r_t+\gamma{\cdot}q_{t+1})$
- 求解价值网络的微分：$d_{\omega,t}=\frac{\partial{q(a_t|s_t;\omega)}}{\partial\omega}|_{\omega=\omega_t}$
- 更新价值网络参数：$\omega_{t+1}=\omega_t - \alpha{\cdot}\delta_t{\cdot}d_{\omega,t}$
- 求解策略网络的微分：$d_{\theta,t}=\frac{\partial\log(a_t|s_t,\theta)}{\partial\theta}|_{\theta=\theta_t}$
- 更新策略网络：$\theta_{t+1}=\theta_t+\beta{\cdot}q_t{\cdot}d_{\theta,t}$

## AlphaGo

### 训练和执行

`AlphaGo` 的训练步骤如下：

- 使用监督学习的方法模仿人类棋谱训练出一个策略网络
- 使用策略梯度算法进一步训练策略网络，让策略网络进行自我博弈，使用胜负结果来训练策略网络
- 训练出一个价值网络，该步使用策略网络做自我博弈，使用胜负结果作为目标，让价值网络来拟合该目标

`alphaGo` 使用训练好的策略网络和价值网络来配合**蒙特卡洛树搜索**进行实际的操作执行

### 策略网络

`AlphaGo` 使用一个 `19*19*48` 的张量来表示棋盘当前的状态，将词状态向量作为策略网络的输入，策略网络均为卷积层，策略网络的输出是一个维度为 `19*19` 的矩阵，输出是每一个动作的概率，代表了棋盘上每一个位置

- 策略网络的随机初始化会大大增加网络的摸索过程，`AlphaGo` 利用人的知识来学习一个初步的策略网络，这种模仿学习是一种简单的分类或回归网络，不是强化学习，模仿学习的步骤如下：
  - 观测到状态 $s_t$
  - 策略网络做出预测 $p_t=[\pi(1|s_t,\theta),...,\pi(361|s_t,\theta)]\in(0,1)^{361}$
  - 专家的动作是 $a_t^\star=281$，定义 $y_t\in{0,1}^{361}$ 是动作 $a_t^\star$ 的独热编码向量
  - 计算损失函数 $Loss=CrossEntropy(y_t,p_t)$
  - 使用梯度下降法来更新策略网络
- 模仿学习在训练数据集中没有出现过的样本上表现较差，使用策略梯度继续训练策略网络，可以使得策略网络更加强大，强化学习会探索更多的状态，并使用奖励来指导某种状态下该做什么动作
  - 假设一局游戏在第 $T$ 步终止
  - 当游戏没结束时，奖励定义为：$r_1=r_2=r_3=...=r_{T-1}=0$，当游戏结束时，奖励定义为：$r_T=+1(winner)$ 或 $r_T=-1(loser)$
  - 奖励回报定义为：$u_t=\Sigma_{i=t}^{T}r_i$
  - 所以，游戏获胜时返回：$u_1=u_2=...=u_T=+1$，游戏失败时返回：$u_1=u_2=...=u_T=-1$
- 策略梯度的求解步骤
  - 使用 $\frac{\partial\log\pi(a_t|s_t;\theta)}{\partial\theta}{{\cdot}Q_\pi(s_t,a_t)}$ 来近似策略梯度
  - 其中，动作价值函数 $Q_\pi(s_t,a_t)=E[U_t|s_t,a_t]$
  - 可以使用观测到的回报 $u_t$ 来近似 $Q_\pi(s_t,a_t)$
  - 则策略梯度可以近似为：$\frac{\partial\log\pi(a_t|s_t;\theta)}{\partial\theta}{\cdot}u_t$
- 训练策略网络的重复过程
  - 使用两个策略网络 `player` 和 `opponent` 对弈来玩一局游戏直到结束
  - 得到一个轨迹：$s_1,a_1,s_2,a_2,...,s_T,a_T$
  - 在一局游戏结束的时候，更新策略网络 `player` 的参数
    - `player` 的奖励回报：$u_1=u_2=...=u_T.(Either +1 or -1)$
    - 近似策略梯度的和：$g_\theta=\Sigma_{i=t}^{T}\frac{\partial\log\pi(a_t|s_t;\theta)}{\partial\theta}{\cdot}u_t$
    - 更新策略网络：$\theta\leftarrow{\theta+\beta\cdot{g_\theta}}$
- 使用策略网络来下棋
  - 此时，已经学习好了一个策略网络 $\pi$
  - 根据观测到的状态 $s_t$，随机采样动作：$a_t\sim\pi(\cdot|s_t,\theta)$
  - 此时策略网络已经很强了，但还是会有犯错的可能，而每一个小的错误都可能会输掉比赛，使用蒙特卡洛树搜索是一个更好的选择
  - 在进行蒙特卡洛树搜索之前还需要学习一个价值网络

### 价值网络

价值网络 $v(s;\omega)$ 是对状态价值函数 $V_\pi(s)$ 的近似，状态价值函数可表示为：$V_\pi(s)=E[U_t|S_t=s]$，当赢下一局游戏时，$U_t=+1$，当输掉一局游戏时，$U_t=-1$，价值网络可以评价当前棋盘状况的好坏

- 价值网络的更新步骤
  - 使用两个训练好的策略网络 `player` 和 `opponent` 来对弈到结束
    - 如果 `player` 赢了游戏，让 $u_1=u_2=...=u_T=+1$
    - 如果 `player` 输了游戏，让 $u_1=u_2=...=u_T=-1$
  - 计算损失函数：$Loss=\Sigma_{t=1}^{T}\frac{1}{2}[v(s_t;\omega)-u_t]^2$
  - 更新网络参数：$\omega\leftarrow\omega-\alpha{\cdot}\frac{\partial{L}}{\partial{\omega}}$

### 蒙特卡洛树搜索

- 主要思想
  - 随机选择一个动作 $a$
  - 向前执行并判断 $a$ 会导致输还是赢
  - 重复多次执行这个步骤
  - 选择一个分数最高的动作 $a$
- 主要步骤
  - `Selection`：策略网络 `player` 选择一个动作 $a$，这个动作是模拟出来的，不会真正执行
  - `Expansion`：策略网络 `opponent` 执行一个动作，同时状态会更新，这是通过策略网络模拟出的动作
  - `Evaluation`：执行状态价值函数，得到分数 $v$，玩游戏直到结束得到奖励 $r$，定义 $\frac{v+r}{2}$ 为动作 $a$ 的分数
  - `Backup`：使用分数 $ \frac{v+r}{2}$ 去更新动作价值
- `Selection`
  - 为所有可能的动作 $a$ 打一个分数：$score(a)=Q(a)+\eta{\cdot}\frac{\pi(a|s_t;\theta)}{1+N(a)}$
    - $Q(a)$：通过蒙特卡洛树搜索计算出来的动作价值
    - $\pi(a|s_t;\theta)$：策略网络对动作的打分
    - $N(a)$：在给定状态 $s_t$ 时，动作 $a$ 被选择的次数
  - 分数 $score(a)$ 最高的动作会被选中
- `Expansion`
  - 给定动作 $a_t$，对手执行动作 $a_t^{’}$  会产生新的状态 $s_{t+1}$
  - 对手所执行的动作 $a_t^{’}$ 是按照策略网络随机抽取的，$a_t^{’}\sim\pi(\cdot|s_t^{’};\theta)$，这里的 $s_t^{’}$ 是对手观察到的状态
  - 由于状态转移函数 $p(s_{t+1}|s_t,a_t)$ 是未知的，使用策略函数 $\pi$ 作为状态转移函数 $p$
- `Evaluation`
  - 执行快速走子网络直到游戏结束
    - 策略网络 `player` 的动作：$a_k\sim\pi(\cdot|s_k;\theta)$
    - 策略网络 `opponent` 的动作：$a_k\sim\pi(\cdot|s_k^{’};\theta)$
    - 在整局游戏结束是得到奖励 $r_T$，赢得游戏，奖励 $r_T=+1$，输掉游戏，奖励 $r_T=-1$
  - 评估状态 $s_{t+1}$
    - 利用价值网络的输出 $v(s_{t+1};\omega)$ 和奖励 $r_T$ 来评估状态 $s_{t+1}$：$V(s_{t+1})=\frac{1}{2}v(s_{t+1};\omega)+\frac{1}{2}r_T$
- `Backup`
  - 蒙特卡洛树搜索会进行很多次的模拟，$a_t$ 的每一个子节点都会有很多个 $v(s_{t+1})$
  - 更新动作价值：$Q(a_t)=mean(V^{’}(s))$，这个 $Q$ 值在第一步 `selection` 中会被用到
- 决策过程
  - $N(a)$ 代表了动作 $a$ 被选择的次数
  - 在蒙特卡洛树搜索完成之后，选择 $N(a)$ 最大的动作作为实际执行的动作：$a_t=argmax_aN(a)$
- 总结
  - 蒙特卡洛树搜索有4步，`selection`，`expansion`，`evaluation`，`backup`
  - 为了执行一个动作，`alphaGo` 会将这4步重复很多次，计算出每个动作的 $Q(a)$ 和 $N(a)$
  - `alphaGo` 执行 $N(a)$ 最大的动作
  - 当执行下一个动作的时候，`alphaGo` 会再一次做蒙特卡洛树搜索，重新初始化 $Q(a)$ 和 $N(a)$

### AlphaGo Zero

`AlphaGo Zero` 训练的步骤如下：

- 观测到状态 $s_t$
- 通过策略网络预测出动作概率：$p=[\pi(a=1|s_t;\theta),...,\pi(a=361|s_t;\theta)]\in{R^{361}}$
- 通过蒙特卡洛树预测：$n=normalize[N(a=1),N(a=2),...,N(a=361)]\in{R^{361}}$
- 计算损失函数：$Loss=CrossEntropy(n,p)$
- 使用 $\frac{\partial{L}}{\partial{\theta}}$ 去更新参数 $\theta$

## 蒙特卡洛算法

利用蒙特卡洛算法可以近似期望，这在统计学和机器学习中及其重要

- 期望的定义
  - 定义 $X$ 是一个 $d$ 维的随机变量
  - 定义 $p(x)$ 是 $X$ 的概率密度函数
  - $f(x)$ 是变量为 $x$ 的任意函数
  - 则函数 $f(x)$ 的期望为：$E_{X\sim{p}}[f(X)]={\int_{Rd}f(x){{\cdot}p(x)dx}}$
-  蒙特卡洛近似求期望主要有以下几步：
  - 按照概率密度函数 $p(x)$ 来随机抽样得到样本 $x_1,x_2,...,x_n$
  - 计算 $Q_n=\frac{1}{n}\Sigma_{i=1}^nf(x_i)$
  - 返回 $Q_n$ 作为期望 $E_{X\sim{p}}[f(X)]$ 的估计
- 总结
  - 样本数量越大，蒙特卡洛的估计越准确
  - 蒙特卡洛算法是一种数值算法，靠随机样本来对目标做近似
  - 蒙特卡洛的结果经常是错的，但十分接近于真实结果

## 价值学习中的TD算法

### Sarsa

#### TD Target的推导

`TD Target` 的推导步骤如下：

- $U_t=R_t+\gamma{\cdot}U_{t+1}$
- 假设 $R_t$ 依赖于 $(S_t,A_t,S_{t+1})$，则 $Q_\pi(s_t,a_t)=E[U_t|s_t,a_t]=E[R_t+\gamma{\cdot}U_{t+1}|s_t,a_t]=E[R_t|s_t,a_t]+\gamma{\cdot}E[U_{t+1}|s_t,a_t]=E[R_t|s_t,a_t]+\gamma{\cdot}E[Q_\pi(S_{t+1},A_{t+1})|s_t,a_t]$

- 定义等式：$Q_\pi(s_t,a_t)=E[R_t+\gamma{\cdot}Q_\pi(S_{t+1},A_{t+1})]$
- 使用蒙特卡洛近似可得：$Q_\pi(s_t,a_t){\approx}r_t+\gamma{\cdot}Q_\pi(s_{t+1},a_{t+1}){\approx}y_t$

`TD Learning`：鼓励 $Q_\pi(s_t,a_t)$ 去接近 $y_t$

#### 表格形式的Sarsa

- 观察到一个状态转移序列 $(s_t,a_t,r_t,s_{t+1})$
- 采样得到 $a_{t+1}\sim\pi(\cdot|s_{t+1})$，其中，$\pi$ 是策略函数
- 计算 `TD Target`：$y_t=r_t+\gamma{\cdot}Q_\pi(s_{t+1},a_{t+1})$
- 计算 `TD error`：$\delta_t=Q_\pi(s_t,a_t)-y_t$
- 更新：$Q_\pi(s_t,a_t)\leftarrow{Q_\pi(s_t,a_t)}-\alpha{\cdot}\delta_t$

#### 神经网络形式的Sarsa

通过价值网络 $q(s,a;\omega)$ 来拟合动作价值函数 $Q_\pi(s,a)$，$q(s,a;\omega)$ 被作为评价者来评价动作网络的好坏，其中待学习的参数是 $\omega$

此时，`sarsa` 的学习步骤如下：

- 计算 `TD Target`：$y_t=r_t+\gamma{\cdot}q(s_{t+1},a_{t+1})$
- 计算 `TD error`： $\delta_t=q(s_t,a_t)-y_t$
- 计算损失：$\delta_t^2 / 2$
- 计算梯度：$\frac{\partial\delta_t^2 / 2}{\partial\omega}=\delta_t\cdot{\frac{\partial{q(s_t,a_t;\omega)}}{\partial\omega}}$
- 利用梯度下降更新网络参数：$\omega\leftarrow\omega-\alpha{\cdot}\delta_t{\cdot\frac{\partial{q(s_t,a_t;\omega)}}{\partial{\omega}}}$

### Q-Learning

`Q-learning` 算法步骤：

- `Q-Learning` 是训练最优的动作价值函数 $Q^\star(s,a)$
- `TD Target`：$y_t=r_t+\gamma{\cdot}max_aQ^\star(s_{t+1},a)$
- `DQN` 的更新使用到了 `Q-Learning`

#### TD Target的推导

- 对于所有的策略函数 $\pi$，均有：$Q_\pi(s_t,a_t)=E[R_t+\gamma{\cdot}Q_\pi(S_{t+1},A_{t+1})]$
- 如果 $\pi$ 是最优的策略 $\pi^\star$，那么：$Q_{\pi^\star}(s_t,a_t)=E[R_t+\gamma{\cdot}Q_{\pi^\star}(S_{t+1},A_{t+1})]$
- 通常将 $ Q_{\pi^\star}$ 记作 $ Q^\star$，所以可得：$Q^\star(s_t,a_t)=E[R_t+\gamma{\cdot}Q^\star(S_{t+1},A_{t+1})]$

- 假设动作 $A_{t+1}$ 是最优动作，则 $A_{t+1}=argmax_aQ^\star(S_{t+1},a)$，因此：$Q^\star(S_{t+1},A_{t+1})=max_aQ^\star(S_{t+1},a)$，继而可得 $Q^\star(s_t,a_t)=E[R_t+\gamma{\cdot}max_aQ^\star(S_{t+1},a)]$

- 对期望做蒙特卡洛近似可得：$Q^\star(s_t,a_t)\approx{r_t+\gamma{\cdot}max_aQ^\star(s_{t+1},a)}\approx{y_t}$

#### 表格形式的Q-Learning

- 观察到一个状态转移序列 $(s_t,a_t,r_t,s_{t+1})$
- 计算 `TD Target`：$y_t=r_t+\gamma{\cdot}max_aQ^\star(s_{t+1},a)$
- 计算 `TD error`：$\delta_t=Q^\star(s_t,a_t)-y_t$
- 更新：$Q^\star(s_t,a_t)\leftarrow{Q^\star(s_t,a_t)}-\alpha{\cdot}\delta_t$

#### 神经网络形式的Q-Learning

通过价值网络 $Q(s,a;\omega)$ 来拟合动作价值函数 $Q^{\star}(s,a)$，智能体执行的动作是：$a_t=argmax_aQ(s_t,a;\omega)$，其中待学习的参数是 $\omega$

此时，`Q-Learning` 的学习步骤如下：

- 计算 `TD Target`：$y_t=r_t+\gamma{\cdot}max_aQ(s_{t+1},a;\omega)$
- 计算 `TD error`： $\delta_t=Q(s_t,a_t;\omega)-y_t$
- 利用梯度下降更新网络参数：$\omega\leftarrow\omega-\alpha{\cdot}\delta_t{\cdot\frac{\partial{Q(s_t,a_t;\omega)}}{\partial{\omega}}}$

### 多步的TD算法

`Sarsa TD Target`：$y_t=r_t+\gamma{\cdot}Q_\pi(s_{t+1},a_{t+1})$

`Q-Learning TD Target`：$y_t=r_t+\gamma{\cdot}max_aQ^\star(s_{t+1},a)$

使用多个奖励 $r_t$ 可能会使得算法表现更好

定义：$U_t=\Sigma_{i=0}^{m-1}\gamma^i{\cdot}R_{t+i}+\gamma^m{\cdot}U_{t+m}$，则:

 `Sarsa TD Target`：$y_t=\Sigma_{i=0}^{m-1}\gamma^i{\cdot}r_{t+i}+\gamma^m{\cdot}Q_{\pi}(s_{t+m},a_{t+m})$

`Q-Learning TD Target`：$y_t=\Sigma_{i=0}^{m-1}\gamma^i{\cdot}r_{t+i}+\gamma^m{\cdot}max_aQ^\star(s_{t+m},a)$

> 当 $m=1$ 时，多步 TD Target 会变为标准的 TD Target

## 价值学习的高级技巧

### 经验回放

经验回放的优势在于：

- 可以重复利用经验，避免经验浪费
- 消除经验之间的相关性

使用经验回放的 `TD` 算法步骤如下：

- 计算 $L(\omega)=\frac{1}{T}\Sigma_{t=1}^T\frac{\delta_t^2}{2}$ 
- 随机梯度下降更新参数 $\omega$
  - 从经验回放池中随机采样一个或 `minibatch `个 `transition`，$(s_i,a_i,r_i,s_{i+1})$
  - 计算 `TD error` $\delta_t$
  - 计算随机梯度：$g_i=\frac{\partial\delta_i^2/2}{\partial\omega}=\delta_i{\cdot}\frac{\partial{Q(s_i,a_i;\omega)}}{\partial{\omega}}$
  - 随机梯度下降更新参数：$\omega\leftarrow\omega-\alpha{\cdot}g_i$

#### 优先经验回放

基本想法：使用**非均匀抽样**代替均匀抽样

- 方式一：$p_t\propto|\delta_t|+\varepsilon$
- 方式二：$p_t\propto\frac{1}{rank(t)}$
  - 按照 $|\delta_t|$ 对 `transition` 进行排序
  - $rank(t)$ 是第 `t` 个 `transition` 的序号
- 总的来说，越大的 $|\delta_t|$，应该赋予一个更高的优先级

均匀抽样时，所有的 `transition` 应该有相同的学习率，非均匀抽样时，学习率 $\alpha$ 应该根据抽样概率进行调整，学习率可以修改为：$\alpha{\cdot}(np_t)^{-\beta}$，其中：$\beta\in(0,1)$，总的来说，抽样概率越高的 `transition` ，其学习率越低，$\beta$ 是可以调整的超参数，一开始可以设置为一个很小的值，在实验过程中逐渐增大到 1

如何更新 `TD error`：

- 为每一个 `transition` 关联一个 `TD error` $\delta_t$
- 如果 `transition` 是新收集的，其 $\delta_t$ 的值可以设置为最大，使其拥有最高的优先级
- 当每一个 $(s_t,a_t,r_t,s_{t+1})$ 被从 `buffer` 中选择时，需要更新 $\delta_t$

> 越大的 $|\delta_t|$ 拥有越高的抽样概率，但其学习率越小，用于抵消大的抽样概率所带来的偏差

### 高估问题、目标网络和 Double DQN

`TD Learning` 使得 `DQN` 对动作价值高估的原因如下：

- 计算 `TD Target`  时使用到了最大化，此时 `TD Target` 会大于真实的动作价值
- 计算 `TD Target` 时用到了自身网络，`Bootstrapping` 也会导致高估

> 非均匀的高估往往会产生问题

解决方法：定义目标网络 `Target Network` 去计算 `TD Target`

- 使用目标网络计算 `TD Target`：$y_t=r_t+\gamma{\cdot}max_aQ(s_{t+1},a;\omega^-)$
- 计算 `TD error`：$\delta_t=Q(s_t,a_t;\omega)-{y_t}$
- 使用随机梯度下降更新参数：$\omega\leftarrow\omega-\alpha{\cdot}\delta_t{\cdot}\frac{\partial{Q(s_t,a_t;\omega)}}{\partial\omega}$

`Double DQN` 解决高估问题时，选择最优动作和利用目标网络计算 `TD Target` 是使用了两个神经网络进行的，使用`DQN` 选择最优动作时，$a^\star=argmax_aQ(s_{t+1},a;\omega)$，计算 `TD Target` 时，$y_t=r_t+\gamma{\cdot}Q(s_{t+1},a^\star;\omega^-)$

> `Double DQN` 没有彻底解决高估问题

- 目标网络可以在一定程度上避免`bootstrapping`，但不能完全避免，因为 $\omega-$ 依赖于 $\omega$

- `Double DQN` 可以缓解最大化导致的高估

### Dueling Network

- 折扣回报：$U_t=R_t+\gamma{\cdot}R_{t+1}+\gamma^2{\cdot}R_{t+2}+\gamma^3{\cdot}R_{t+3}+...$
- 动作价值函数：$Q_\pi(a_t,a_t)=E[U_t|S_t=s_t,A_t=a_t]$
- 状态价值函数：$V_\pi(s_t)=E_A[Q_\pi(s_t,A)]$
- 最优动作价值函数：$Q^\star(s,a)=max_\pi{Q_\pi(s,a)}$，评价了在状态 $s$ 的情况下，做出动作 $a$ 的好坏
- 最优状态价值函数：$V^\star(s)=max_\pi{V_\pi(s)}$
- 最优优势函数：$A^\star(s,a)=Q^\star(s,a)-V^\star(s)$，将 $V^\star(s)$ 作为 `baselne`，$A^\star(s,a)$ 的意思是动作 $a$ 相对于 `baseline` 的优势，动作越好，优势越大

#### 优势函数的属性

定理一：$V^\star(s)=max_aQ^\star(s,a)$

由于：$A^\star(s,a)=Q^\star(s,a)-V^\star(s)$，所以：$max_aA^\star(s,a)=max_aQ^\star(s,a)-V^\star(s)=0$

由此可得定理二：$Q^\star(s,a)=V^\star(s)+A^\star(s,a)-max_aA^\star(s,a)$

#### 搭建 Dueling Nerwork

- 使用神经网络 $A(s,a;\omega^A)$ 对优势函数 $A^\star(s,a)$ 近似
- 使用神经网络 $V(s;\omega^V)$ 对状态价值函数 $V^\star(s)$ 近似
- 因此，使用神经网络近似 $Q^\star(s,a)$ 可得：$Q(s,a;\omega^A,\omega^V)=V(s;\omega^V)+A(s,a;\omega^A)-max_aA(s,a;\omega^A)$，这个神经网络被称为 `dueling network`，搭建的图示如下：

![dueling_network](.\images\dueling_network.png)

- `Dueling network` $Q(s,a;\omega)$ 是对 $Q^\star(s,a)$ 的近似
- `Dueling network` 参数的学习过程和 `DQN` 完全一致，一些高级技巧依旧适用
- 实验中发现将 `Dueling network` 定义为：$Q(s,a;\omega^A,\omega^V)=V(s;\omega^V)+A(s,a;\omega^A)-mean_aA(s,a;\omega^A)$ 会更加有效

## 策略梯度的 Baseline

- 使用策略网络 $\pi(a|s;\theta)$ 来控制智能体
- 状态价值函数 $V_\pi(s)=E_{{A\sim\pi}}[Q_\pi(s,A)]=\Sigma_a\pi(a|s;\theta){\cdot}Q_\pi(s,a)$
- 计算策略梯度：$\frac{\partial{V_\pi(s)}}{\partial\theta}=E_{A\sim\pi}[\frac{\partial{\ln\pi(A|s;\theta)}}{\partial\theta}{\cdot}Q_\pi(s,A)]$

`Baseline` 是不依赖 `A` 的函数，且 $E_{A\sim\pi}[\frac{\partial{\ln\pi(A|s;\theta)}}{\partial\theta}{\cdot}b]=0$，由此可得：$\frac{\partial{V_\pi(s)}}{\partial\theta}=E_{A\sim\pi}[\frac{\partial{\ln\pi(A|s;\theta)}}{\partial\theta}{\cdot}Q_\pi(s,A)]-E_{A\sim\pi}[\frac{\partial{\ln\pi(A|s;\theta)}}{\partial\theta}{\cdot}b]=E_{A\sim\pi}[\frac{\partial{\ln\pi(A|s;\theta)}}{\partial\theta}{\cdot}{Q_\pi(s,A)-b}]$

定义：$\frac{\partial{V_\pi(s)}}{\partial\theta}=E_{A\sim\pi}[\frac{\partial{\ln\pi(A|s;\theta)}}{\partial\theta}{\cdot}{Q_\pi(s,A)-b}]=g(A_t)$

- 随机采样 $a_t=\pi({\cdot}|s_t;\theta)$ 并计算 $g(a_t)$
- $g(a_t)$ 是策略梯度的无偏估计：$E_{A_t\sim\pi}[g(A_t)]=\frac{\partial{V_\pi(s_t)}}{\partial\theta}$
- 随机策略梯度可计算为：$g(a_t)=\frac{\partial{\ln\pi(A|s;\theta)}}{\partial\theta}{\cdot}{Q_\pi(s,A)-b}$
- 使用随机梯度上升来更新参数：$\theta\leftarrow\theta+\beta{{\cdot}g(a_t)}$

需要注意的是：

- 如果 $b$ 不依赖于 $A_t$，无论 $b$ 是什么，策略梯度 $E_{A_t\sim\pi}[g(A_t)]$ 始终是相同的
- 但是 $b$ 会影响 $g(a_t)$
- 一个好的 $b$ 会产生一个小的方差，从而加快收敛速度

常见的 `Baseline`：

- $b=0$，这是标准的策略梯度
- $b=V_\pi(s_t)$，$V_\pi(s_t)=E_{A_t}[Q_\pi(s_t,a_t)]$，$V_\pi(s_t)$ 非常接近于 $Q_\pi(s_t,A_t)$，用 $V_\pi(s_t)$ 做 `basebine` 很合适

### Reinforce 方法

- 策略梯度：$\frac{\partial{\ln\pi(A|s;\theta)}}{\partial\theta}{\cdot}{Q_\pi(s,A)-	V_\pi(s_t)}$
- 随机策略梯度：$\frac{\partial{\ln\pi(a|s;\theta)}}{\partial\theta}{\cdot}{Q_\pi(s,a)-V_\pi(s_t)}$
- 由于：$Q_\pi(s_t,a_t)=E[U_t|s_t,a_t]$，所以，使用蒙特卡洛近似 $Q_\pi(s_t,a_t)\approx{u_t}$，称作 `reinforce`
  - 观察到一条完整的轨迹：$s_t,a_t,s_{t+1},a_{t+1},r_{t+1},...,s_n,a_n,r_n$
  - 计算奖励回报：$u_t=\Sigma_{i=t}^n{\gamma^{i-t}{\cdot}r_i}$
  - $u_t$ 是 $Q_\pi(s_t,a_t)$ 的无偏估计
- 将状态价值函数 $V_\pi(s)$ 用价值网络 $v(s;\omega)$ 近似

#### 训练策略网络

由于：$\frac{\partial{V_\pi(s)}}{\partial\theta}\approx{\frac{\partial{\ln\pi(a_t|s_t;\theta)}}{\partial\theta}{\cdot}{(u_t-v(s_t;\omega))}}$

所以：$\theta\leftarrow\theta+\beta{\cdot}{\frac{\partial{\ln\pi(a_t|s_t;\theta)}}{\partial\theta}{\cdot}{(u_t-v(s_t;\omega))}}=\theta-\beta{\cdot}{\delta_t}{\cdot}{\frac{\partial{\ln\pi(a_t|s_t;\theta)}}{\partial\theta}}$

#### 训练价值网络

- 定义预测误差：$\delta_t=v(s_t;\omega)-u_t$
- 计算梯度：$\frac{\partial{\delta_t^2}}{\partial\omega}=\delta_t{\cdot}\frac{\partial{v(s_t;\omega)}}{\partial\omega}$
- 梯度下降更新参数：$\omega\leftarrow\omega-\alpha{\cdot}\delta_t{\cdot}\frac{\partial{v(s_t;\omega)}}{\partial\omega}$

#### 总结

- 玩一局游戏得到整个轨迹：$s_t,a_t,s_{t+1},a_{t+1},r_{t+1},...,s_n,a_n,r_n$
- 计算：$u_t=\Sigma_{i=t}^n{\gamma^{i-t}{\cdot}r_i}$ 以及 $\delta_t=v(s_t;\omega)-u_t$
- 更新策略网络：$\theta\leftarrow=\theta-\beta{\cdot}{\delta_t}{\cdot}{\frac{\partial{\ln\pi(a_t|s_t;\theta)}}{\partial\theta}}$
- 更新价值网络：$\omega\leftarrow\omega-\alpha{\cdot}\delta_t{\cdot}\frac{\partial{v(s_t;\omega)}}{\partial\omega}$
- 从 $t=1,...,n$，不断重复 2~4 步，进行网络的训练和更新

### A2C 方法

使用策略网络 $\pi(a|s;\theta)$ 来近似策略函数 $\pi(a|s)$，用来控制智能体；使用价值网络 $v(s;\omega)$ 来近似状态价值函数 $V_\pi(s)$ ，用来评价当前状态 $s$ 的好坏

`A2C` 的训练步骤如下：

- 每一轮观测到一个 $(s_t,a_t,r_t,s_{t+1})$
- 计算 `TD Target` ：$y_t=r_t+\gamma{\cdot}v(s_{t+1};\omega)$
- 计算 `TD error`：$\delta_t=v(s_t;\omega)-y_t$
- 更新策略网络参数：$\theta\leftarrow\theta-\beta{\cdot}\delta_t{\cdot}\frac{\partial{ln{\pi(a_t|s_t;\theta)}}}{\partial\theta}$
- 更新价值网络参数：$\omega\leftarrow\omega-\alpha{\cdot}\delta_t{\cdot}\frac{\partial{v(s_t;\omega)}}{\partial\omega}$

#### 动作价值函数的性质

- 因为：$Q_\pi(s_t,a_t)=E_{S_{t+1},A_{t+1}}[R_t+\gamma{\cdot}Q_\pi(S_{t+1},A_{t+1})]$，所以，$Q_\pi(s_t,a_t)=E_{S_{t+1}}[R_t+\gamma{\cdot}E_{A_{t+1}}[Q_\pi(S_{t+1},A_{t+1})]]=E_{S_{t+1}}[R_t+\gamma{\cdot}V_\pi(s_{t+1})]$
- 可得到 `等式一`：$Q_\pi(s_t,a_t)=E_{S_{t+1}}[R_t+\gamma{\cdot}V_\pi(S_{t+1})]$
- 继而可得：$V_\pi(s_t)=E_{A_{t}}[Q_\pi(s_t,a_t)]=E_{A_{t}}[E_{S_{t+1}}[R_t+\gamma{\cdot}V_\pi(S_{t+1})]]$
- 由此可得 `等式二`：$V_\pi(s_t)=E_{A_{t},S_{t+1}}[R_t+\gamma{\cdot}V_\pi(S_{t+1})]$

#### 期望的蒙特卡洛估计

- 假设我们知道 $(s_t,a_t,r_t,s_{t+1})$
- `等式一`的无偏估计可写为：$Q_\pi(s_t,a_t){\approx}r_t+\gamma{\cdot}V_\pi(s_{t+1})$
- `等式二`的无偏估计可写为：$V_\pi(s_t){\approx}r_t+\gamma{\cdot}V_\pi(s_{t+1})$

#### 策略网络的更新

- 近似的随机策略梯度公式可写为：$g(a_t)\approx{\frac{\partial{ln{\pi(a_t|s_t;\theta)}}}{\partial\theta}}{\cdot(r_t+\gamma{\cdot}V_\pi(s_{t+1})-V_\pi(s_t))}$
- 使用神经网络 $v(s;\omega)$ 来近似 $V_\pi(s;\omega)$，则可得：$g(a_t)\approx{\frac{\partial{ln{\pi(a_t|s_t;\theta)}}}{\partial\theta}}{\cdot(r_t+\gamma{\cdot}v(s_{t+1};\omega)-v(s_t;\omega))}$
- 令：$y_t=r_t+\gamma{\cdot}v(s_{t+1};\omega)$，则使用随机梯度下降来更新参数 $\theta$：$\theta\leftarrow\theta+\beta{\cdot}\frac{\partial{ln{\pi(a_t|s_t;\theta)}}}{\partial\theta}{\cdot(y_t-v(s_t;\omega))}$

#### 价值网络的更新

- 使用神经网络 $v(s;\omega)$ 来近似 $V_\pi(s;\omega)$，则 $v(s_t;\omega){\approx}r_t+\gamma{\cdot}v(s_{t+1};\omega)\approx{y_t}$，算法鼓励 $v(s_t;\omega)$ 去接近 $y_t$
- 计算误差：$\delta_t=v(s_t;\omega)-y_t$
- 计算梯度：$\frac{\partial{\delta_t^2}}{\partial\omega}=\delta_t{\cdot}\frac{\partial{v(s_t;\omega)}}{\partial\omega}$
- 梯度下降更新参数：$\omega\leftarrow\omega-\alpha{\cdot}\delta_t{\cdot}\frac{\partial{v(s_t;\omega)}}{\partial\omega}$

#### 直观解释

对应随机策略梯度而言：$g(a_t)\approx{\frac{\partial{ln{\pi(a_t|s_t;\theta)}}}{\partial\theta}}{\cdot(r_t+\gamma{\cdot}v(s_{t+1};\omega)-v(s_t;\omega))}$，$r_t+\gamma{\cdot}v(s_{t+1};\omega)$ 和 $v(s_t;\omega))$ 都可以评价状态 $s_t$ 的好坏，但是 $r_t+\gamma{\cdot}v(s_{t+1};\omega)$ 依赖于动作 $a_t$，而 $v(s_t;\omega))$ 不依赖于动作 $a_t$，所以二者的差可以反应出动作 $a_t$ 带来的优势，因此可以被用来训练策略网络，`A2C` 的训练过程如下：

![a2c](.\images\a2c.png)

### Reinforce 和 A2C 的异同点

`Reinforce` 和 `A2C`  的区别主要在于 `TD Target`

- `Reinforce` 的**多步** `TD Target` 可以表示为：$y_t=\Sigma_{i=0}^{m-1}\gamma^i{\cdot}r_{t+i}+\gamma^m{\cdot}v(s_{t+m};\omega)$
- `A2C` 的奖励回报可以表示为：$u_t=\Sigma_{i=t}^n{\gamma^{i-t}{\cdot}r_t}$

`Reinforce` 是 `A2C` 的一种特例，使用了一个 `Episode` 的所有奖励

## 连续控制问题

连续控制用于解决连续动作空间的问题，将连续动作空间离散会导致维度灾难

### 确定性策略梯度

- 使用确定性策略网络得到确定性动作：$a=\pi(s;\theta)$
- 价值网络 $q(s,a;\omega)$ 输出是一个标量，用于评价价值好坏

#### 价值网络的训练

使用 `TD` 算法更新价值网络

- 观察到一个状态转移轨迹：$(s_t,a_t,r_t,s_{t+1})$
- 使用价值网络得到 $t$ 时刻的价值：$q_t=q(s_t,a_t;\omega)$
- 使用价值网络得到 $t+1$ 时刻的价值：$q_{t+1}=q(s_{t+1},a_{t+1}^{’};\omega)$，其中，$a_{t+1}^{’}=\pi(s_{t+1};\theta)$
- 计算 `TD error`：$\delta_t=q_t-(r_t+\gamma{\cdot}q_{t+1})$
- 更新价值网络参数：$\omega\leftarrow\omega-\alpha{\cdot}\delta_t{\cdot}\frac{\partial{q(s_t,a_t;\omega)}}{\partial\omega}$

#### 策略网络的训练

- 训练目标：增加 $q(s,a;\omega)$，其中，$a=\pi(s;\theta)$
- 确定性策略梯度：$g=\frac{\partial{q(s,\pi(s;\theta);\omega)}}{\partial\theta}=\frac{\partial{a}}{\partial{\theta}}{\cdot}\frac{\partial{q(s,a;\omega)}}{\partial{a}}$
- 使用梯度下降更新参数：$\theta\leftarrow\theta+\beta{\cdot{g}}$

#### 改进策略

使用目标网络对确定性策略梯度做改进

- 价值网络对 $t$ 时刻的价值做预测：$q_t=q(s_t,a_t;\omega)$
- 使用目标网络对 $t+1$ 时刻的价值做预测：$q_{t+1}=q(s_{t+1},a_{t+1}^{’};\omega^-)$，其中：$a_{t+1}^{’}=\pi(s_{t+1};\theta^-)$，$\theta^-$ 代表了目标网络的参数，目标网络和原始网络有相同的网络结构，但是网络参数不同

#### 完整实现步骤

- 策略网做决策：$a=\pi(s;\theta)$
- 使用确定性策略梯度更新策略网络参数：$\theta\leftarrow\theta+\beta{\cdot{\frac{\partial{a}}{\partial{\theta}}{\cdot}\frac{\partial{q(s,a;\omega)}}{\partial{a}}}}$
- 使用价值网络计算价值：$q_t=q(s,a;\omega)$
- 使用目标网络 $\pi(s;\theta^-)$ 和 $q(s,a;\omega^-)$ 计算 $q_{t+1}$
- 计算 `TD error`：$\delta_t=q_t-(r_t+\gamma{\cdot}q_{t+1})$
- 使用 `TD` 算法更新价值网络：$\omega\leftarrow\omega-\alpha{\cdot}\delta_t{\cdot}\frac{\partial{q(s_t,a_t;\omega)}}{\partial\omega}$

#### 目标网络的更新

- 设置超参数：$\tau\in(0,1)$
- 使用加权平均更新目标函数网络参数
  - $\omega^-\leftarrow\omega+(1-\tau){\cdot}\omega^-$
  - $\theta^-\leftarrow\theta+(1-\tau){\cdot}\theta^-$

#### 其他改进措施

- 使用目标网络
- 使用经验回放
- 使用多步 `TD Target`

#### 随机策略和确定性策略的对比

|      | 随机策略                     | 确定性策略       |
| ---- | ---------------------------- | ---------------- |
| 策略 | $\pi(a|s;\theta)$            | $\pi(s;\theta)$  |
| 输出 | 动作空间中动作的概率分布     | 确定动作 $a$     |
| 控制 | 从概率分布中随机采样一个动作 | 直接使用输出 $a$ |
| 应用 | 离散控制问题                 | 连续控制问题     |

深度确定性策略梯度的实现步骤如下：

![DPG](.\images\DPG.png)

### 随机策略做连续控制

回顾策略梯度：$\frac{\partial{V_\pi(s)}}{\partial{\theta}}=E_{A\sim\pi}[\frac{\partial{\ln\pi(A|s;\theta)}}{\partial\theta}{\cdot}{Q_\pi(s,A)]}$

随机策略梯度：令 $g(a)=\frac{\partial{\ln\pi(a|s;\theta)}}{\partial\theta}{\cdot}{Q_\pi(s,a)}$，其中 $a\sim\pi({\cdot|s;\theta})$

- 自由度是1的情况：
  - 令 $\mu$ 和 $\sigma$ 都是状态 $s$ 的函数
  - 用均值为 $\mu$ 和方差为 $\sigma$ 的正态分布作为策略函数：$\pi(a|s)=\frac{1}{\sqrt{6.28}{\cdot}\sigma}{\cdot}exp(-\frac{(a-\mu)^2}{2\sigma^2})$ 
- 自由度大于1的情况：
  - 令 $\mu_i$ 和 $\sigma_i$ 分别是 $\mu(s)$ 和 $\sigma(s)$ 的函数
  - 策略函数可以写成连乘的形式：$\pi(a|s)=\Pi_{i=1}^d\frac{1}{\sqrt{6.28}{\cdot}\sigma}{\cdot}exp(-\frac{(a_i-\mu_i)^2}{2\sigma_i^2})$
  - 问题： $\mu$ 和 $\sigma$ 都是未知的
  - 解决办法：将均值 $\mu(s)$ 近似为函数 $\mu(s;\theta^{\mu})$，将方差 $\rho_i=ln{\sigma_i^2}$，其中 $i=1,...,d$ 近似为 $\rho(s;\theta^{\rho})$

连续型控制的求解步骤：

- 观察到一个状态 $s$
- 使用神经网络计算均值 $\widehat\mu=\mu(s;\theta^\mu)$ 和方差对数 $\widehat\rho=\rho(s;\theta^\rho)$
- 对于 $i=1,...,d$，计算 $\widehat\sigma_i^2=exp(\widehat{\rho_i})$
- 对于 $i=1,...,d$，通过 $a_i\sim{N(\widehat{\mu}_i,\widehat\sigma_i^2)}$ 随机采样动作 $a$

#### 策略网络的训练

策略函数的自然对数可以表示为：$ln\pi(a|s;\theta^\mu,\theta^\rho)=\Sigma_{i=1}^d[-ln{\sigma_i-\frac{(a_i-\mu_i)^2}{2\sigma_i^2})}]+const=\Sigma_{i=1}^d[-{\frac{\rho_i}{2}-\frac{(a_i-\mu_i)^2}{2{\cdot}exp(\rho_i)})}]+const$

设置辅助函数：$f(s,a;\theta)=\Sigma_{i=1}^d[-{\frac{\rho_i}{2}-\frac{(a_i-\mu_i)^2}{2{\cdot}exp(\rho_i)})}]$

使用 $\frac{\partial{f}}{\partial{\theta}}$ 计算策略梯度

- 随机策略梯度可以表示为：$g(a)=\frac{\partial{f(s,a;\theta)}}{\partial{\theta}}{\cdot}Q_\pi(s,a)$

- 使用价值网络 $q(s,a;\omega)$ 近似 $Q_\pi$
- 通过 $\theta\leftarrow\theta+\beta{\cdot}\frac{\partial{f(s,a;\theta)}}{\partial{\theta}}{\cdot}q(s,a;\omega)$ 更新策略网络
- 通过 $TD$ 算法更新价值网络 $q(s,a;\omega)$

#### 解决连续控制问题

- 连续空间离散化
- 确定性策略梯度
- 随机策略梯度











